# params.yaml
train:
  model_type: dynamic # This is now controlled by a command-line argument
  test_size: 0.2
  random_state: 42
  target_column: "label"
  handle_imbalance: True # Master switch for imbalance handling
  n_splits: 5

data_transformation:
  numeric_imputer_strategy: "median"
  categorical_imputer_strategy: "most_frequent"

# --- Model Hyperparameters ---

xgboost:
  n_estimators: 150
  max_depth: 5
  learning_rate: 0.1
  objective: 'binary:logistic'
  eval_metric: 'aucpr'
  use_label_encoder: False
  # scale_pos_weight is calculated dynamically in the trainer

lightgbm:
  n_estimators: 200
  learning_rate: 0.05
  num_leaves: 31
  objective: 'binary'
  metric: 'aucpr'
  is_unbalance: True # Controlled by handle_imbalance in the code

logistic_regression:
  solver: 'liblinear'
  penalty: 'l2'
  C: 1.0
  max_iter: 200
  class_weight: 'balanced' # Controlled by handle_imbalance in the code

# --- Hyperparameter Tuning Grids ---
tuning:
  logistic_regression:
    param_grid:
      classifier__solver: ['liblinear', 'saga']
      classifier__penalty: ['l1', 'l2']
      classifier__C: [0.01, 0.1, 1.0]
      classifier__max_iter: [200, 500]

  xgboost:
    param_grid:
      classifier__n_estimators: [50, 100]
      classifier__max_depth: [3, 5]
      classifier__learning_rate: [0.05, 0.1]

  lightgbm:
    param_grid:
      classifier__n_estimators: [50, 100]
      classifier__max_depth: [3, 5]
      classifier__learning_rate: [0.05, 0.1]

# --- Best Hyperparameters (to be updated after tuning) ---
best_params:
  xgboost:
    n_estimators: 150
    max_depth: 5
    learning_rate: 0.1

  lightgbm:
    n_estimators: 200
    learning_rate: 0.05
    num_leaves: 31

  logistic_regression:
    solver: 'saga'
    penalty: 'l1'
    C: 0.1
    max_iter: 500

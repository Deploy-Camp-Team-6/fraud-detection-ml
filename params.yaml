# params.yaml
train:
  model_type: dynamic # This is now controlled by a command-line argument
  test_size: 0.2
  random_state: 42
  target_column: "label"
  handle_imbalance: True # Master switch for imbalance handling
  n_splits: 5

data_transformation:
  numeric_imputer_strategy: "median"
  categorical_imputer_strategy: "most_frequent"

# --- Model Hyperparameters ---

xgboost:
  objective: 'binary:logistic'
  eval_metric: ['aucpr','auc']   # widen metrics
  tree_method: 'hist'
  n_estimators: 1200
  learning_rate: 0.03
  max_depth: 4
  min_child_weight: 10           
  subsample: 0.8                 
  colsample_bytree: 0.8          
  gamma: 1                       
  reg_alpha: 1                   
  reg_lambda: 3                  
  max_delta_step: 1              

lightgbm:
  n_estimators: 200
  learning_rate: 0.05
  num_leaves: 31
  objective: 'binary'
  metric: 'aucpr'
  is_unbalance: True # Controlled by handle_imbalance in the code

logistic_regression:
  solver: 'saga'
  penalty: 'elasticnet'
  l1_ratio: 0.5
  C: 0.5
  max_iter: 500
  class_weight: 'balanced' # Controlled by handle_imbalance in the code

# --- Hyperparameter Tuning Grids ---
tuning:
  logistic_regression:
    param_grid:
      classifier__n_estimators: [600, 1000, 1400]
      classifier__learning_rate: [0.02, 0.03, 0.05]
      classifier__max_depth: [3, 4, 6]
      classifier__min_child_weight: [5, 10, 20]
      classifier__subsample: [0.6, 0.8, 1.0]
      classifier__colsample_bytree: [0.6, 0.8, 1.0]
      classifier__gamma: [0, 1, 5]
      classifier__reg_alpha: [0, 1, 5]
      classifier__reg_lambda: [1, 3, 10]
      classifier__max_delta_step: [0, 1, 5]

  xgboost:
    param_grid:
      classifier__n_estimators: [50, 100, 150]
      classifier__max_depth: [3, 5, 7]
      classifier__learning_rate: [0.01, 0.05, 0.1]

  lightgbm:
    param_grid:
      classifier__n_estimators: [50, 100]
      classifier__max_depth: [3, 5]
      classifier__learning_rate: [0.05, 0.1]

# --- Best Hyperparameters (to be updated after tuning) ---
best_params:
  xgboost:
    n_estimators: 1000          
    max_depth: 8                
    learning_rate: 0.05         
    min_child_weight: 1         
    gamma: 0.1                  
    reg_alpha: 0.1              
    reg_lambda: 1               

  lightgbm:
    n_estimators: 1500          
    learning_rate: 0.02         
    num_leaves: 70              
    reg_alpha: 0.1              
    reg_lambda: 0.1             
    min_child_samples: 10     

  logistic_regression:
    solver: 'saga'
    penalty: 'elasticnet'
    l1_ratio: 0.5
    C: 0.5
    max_iter: 500

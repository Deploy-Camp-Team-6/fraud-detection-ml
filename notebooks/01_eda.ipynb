{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) for Fraud Detection\n",
    "\n",
    "**Objective:** Understand the dataset's characteristics, including class imbalance, feature distributions, correlations, and missing values. This analysis will guide our preprocessing and modeling decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "Load the raw dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw/fraud_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\n--- Data Info ---\")\n",
    "df.info()\n",
    "print(\"\\n--- First 5 Rows ---\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Descriptive Statistics ---\")\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Class Imbalance Analysis\n",
    "This is the most critical step for a fraud detection problem. We need to check how skewed our target variable is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = df['isFraud'].value_counts()\n",
    "class_percentage = df['isFraud'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Fraud Class Distribution:\")\n",
    "print(class_counts)\n",
    "print(\"\\nFraud Class Percentage:\")\n",
    "print(f\"{class_percentage}\")\n",
    "\n",
    "sns.countplot(x='isFraud', data=df)\n",
    "plt.title('Class Distribution (0: Not Fraud, 1: Fraud)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding:** The dataset is highly imbalanced. Fraudulent transactions are a tiny minority. This means accuracy is a poor metric, and we must use metrics like **Precision-Recall AUC**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Missing Values & Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Missing Values per Column ---\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\n--- Total Duplicated Rows ---\")\n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding:** There appear to be no missing values or duplicates in this synthetic dataset. In a real-world scenario, we would need to define an imputation strategy (e.g., mean, median, constant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Distribution and Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing distributions of numerical features\n",
    "numerical_features = df.select_dtypes(include=np.number).columns.tolist()\n",
    "df[numerical_features].hist(bins=30, figsize=(20, 15))\n",
    "plt.suptitle('Distribution of Numerical Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix\n",
    "plt.figure(figsize=(15, 10))\n",
    "# Ensure only numeric columns are used for correlation matrix\n",
    "numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "corr_matrix = numeric_df.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings:**\n",
    "- `oldbalanceOrg` and `newbalanceOrg` are highly correlated.\n",
    "- `oldbalanceDest` and `newbalanceDest` are also highly correlated.\n",
    "These strong correlations suggest potential multicollinearity, but tree-based models like XGBoost are generally robust to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Summary\n",
    "\n",
    "1.  **Imbalance:** The dataset is extremely imbalanced, which is the primary challenge. Our model evaluation strategy must prioritize metrics like AUC-PR, Precision, and Recall.\n",
    "2.  **Features:** We have a mix of numerical and categorical (`type`) features. The `nameOrig` and `nameDest` columns are identifiers and should likely be dropped.\n",
    "3.  **Preprocessing Needs:**\n",
    "    -   Categorical features (`type`) must be one-hot encoded.\n",
    "    -   Numerical features should be scaled to prevent features with large ranges from dominating the model.\n",
    "    -   Identifier columns (`nameOrig`, `nameDest`) should be dropped."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}